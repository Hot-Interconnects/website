---

---
# Principles and Practice of Scalable and Distributed Deep Neural Networks Training and Inference @ HotI 2025

## Abstract

Recent advances in Deep Learning (DL) have led to many exciting challenges and opportunities. Modern DL frameworks such as PyTorch and TensorFlow enable high-performance training, inference, and deployment for various types of Deep Neural Networks (DNNs). This tutorial provides an overview of recent trends in DL and the role of cutting-edge hardware architectures and interconnects in moving the field forward. We will also present an overview of different DNN architectures, DL frameworks, and DL Training and Inference with special focus on parallelization strategies for large models such as GPT, LLaMA, DeepSeek, and ViT. We highlight new challenges and opportunities for communication runtimes to exploit high-performance CPU/GPU architectures to efficiently support large-scale distributed training. We also highlight some of our co-design efforts to utilize MPI for large-scale DNN training on cutting-edge CPU/GPU/DPU architectures available on modern HPC clusters. Throughout the tutorial, we include several hands-on exercises to enable attendees to gain first-hand experience of running distributed DL training on a modern GPU cluster.

## Presenters

- **Dhabaleswar K. (DK) Panda**, Ohio State University
- **Nawras Alnaasan**, Ohio State University

## Tutorial Outline
1. **Introduction**
2. **Deep Learning Frameworks**
3. **Deep Neural Network Training**
4. **Distributed Data-Parallel Training**
5. **Latest Trends in High-Performance Computing Architectures**
6. **Challenges in Exploiting HPC Technologies for DL**
7. **Advanced Distributed Training**
8. **Distributed Inference Solutions**
9. **Open Issues and Challenges**
10. **Conclusion and Q&A**